{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e95f0072",
      "metadata": {
        "id": "e95f0072"
      },
      "source": [
        "\n",
        "# Intro to LangChain\n",
        "\n",
        "LangChain is a popular framework that allow users to quickly build apps and pipelines around **L**arge **L**anguage **M**odels. It can be used to for chatbots, **G**enerative **Q**uestion-**A**nwering (GQA), summarization, and much more.\n",
        "\n",
        "The core idea of the library is that we can _\"chain\"_ together different components to create more advanced use-cases around LLMs. Chains may consist of multiple components from several modules:\n",
        "\n",
        "* **Prompt templates**: Prompt templates are, well, templates for different types of prompts. Like \"chatbot\" style templates, ELI5 question-answering, etc\n",
        "\n",
        "* **LLMs**: Large language models like GPT-3, BLOOM, etc\n",
        "\n",
        "* **Agents**: Agents use LLMs to decide what actions should be taken, tools like web search or calculators can be used, and all packaged into logical loop of operations.\n",
        "\n",
        "* **Memory**: Short-term memory, long-term memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0ef463a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ef463a2",
        "outputId": "79ce4264-24b0-4fe3-bcc2-09e18bd465be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b143a576",
      "metadata": {
        "id": "b143a576"
      },
      "source": [
        "# Using LLMs in LangChain\n",
        "\n",
        "LangChain supports several LLM providers, like Hugging Face and OpenAI.\n",
        "\n",
        "Let's start our exploration of LangChain by learning how to use a few of these different LLM integrations.\n",
        "\n",
        "## Hugging Face\n",
        "\n",
        "We first need to install additional prerequisite libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "badb1fa1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "badb1fa1",
        "outputId": "b38a4c6a-9d98-44b4-eb71-6e96398c647a",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/182.4 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub) (4.0.0)\n",
            "Installing collected packages: huggingface_hub\n",
            "Successfully installed huggingface_hub-0.11.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "823bd325",
      "metadata": {
        "id": "823bd325"
      },
      "source": [
        "For Hugging Face models we need a Hugging Face Hub API token. We can find this by first getting an account at [HuggingFace.co](https://huggingface.co/) and clicking on our profile in the top-right corner > click *Settings* > click *Access Tokens* > click *New Token* > set *Role* to *write* > *Generate* > copy and paste the token below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea18d93",
      "metadata": {
        "id": "7ea18d93"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'HF_API_KEY'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f256aa",
      "metadata": {
        "id": "83f256aa"
      },
      "source": [
        "We can then generate text using a HF Hub model (we'll use `google/flan-t5-x1`) using the Inference API built into Hugging Face Hub.\n",
        "\n",
        "_(The default Inference API doesn't use specialized hardware and so can be slow and cannot run larger models like `bigscience/bloom-560m` or `google/flan-t5-xxl`)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fd3d6ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fd3d6ad",
        "outputId": "39f9bb8b-c116-46a3-e9be-c3b3549789a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "green bay packers\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "# initialize HF LLM\n",
        "flan_t5 = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-xl\",\n",
        "    model_kwargs={\"temperature\":1e-10}\n",
        ")\n",
        "\n",
        "# build prompt template for simple question-answering\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96351d3b",
      "metadata": {
        "id": "96351d3b"
      },
      "source": [
        "If we'd like to ask multiple questions we can by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (`\"question\"`) that is mapped to the question we'd like to ask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9166203a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9166203a",
        "outputId": "f5711d89-de5a-48d0-e815-9f186a84e807"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='green bay packers', generation_info=None)], [Generation(text='184', generation_info=None)], [Generation(text='john glenn', generation_info=None)], [Generation(text='one', generation_info=None)]], llm_output=None)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "res = llm_chain.generate(qs)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16e288c3",
      "metadata": {
        "id": "16e288c3"
      },
      "source": [
        "It is a LLM, so we can try feeding in all questions at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0600bdea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0600bdea",
        "outputId": "c9ff1c1f-2991-4832-d57c-b46cc346ca64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "six\n"
          ]
        }
      ],
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(\n",
        "    template=multi_template,\n",
        "    input_variables=[\"questions\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b123c1da",
      "metadata": {
        "id": "b123c1da"
      },
      "source": [
        "But with this model it doesn't work too well, we'll see this approach works better with different models soon."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15daae92",
      "metadata": {
        "id": "15daae92"
      },
      "source": [
        "## OpenAI\n",
        "\n",
        "Start by installing additional prerequisites:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "60c4379e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60c4379e",
        "outputId": "cb632911-b778-4d09-fd85-68cf12a981d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/77.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7918e43",
      "metadata": {
        "id": "c7918e43"
      },
      "source": [
        "We can also use OpenAI's generative models. The process is similar, we need to\n",
        "give our API key which can be retrieved by signing up for an account on the\n",
        "[OpenAI website](https://openai.com/api/) (see top-right of page). We then pass the API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30f250b8",
      "metadata": {
        "id": "30f250b8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63c711ef",
      "metadata": {
        "id": "63c711ef"
      },
      "source": [
        "If using OpenAI via Azure you should also set:\n",
        "\n",
        "```python\n",
        "os.environ['OPENAI_API_TYPE'] = 'azure'\n",
        "# API version to use (Azure has several)\n",
        "os.environ['OPENAI_API_VERSION'] = '2022-12-01'\n",
        "# base URL for your Azure OpenAI resource\n",
        "os.environ['OPENAI_API_BASE'] = 'https://your-resource-name.openai.azure.com'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a97bdb8e",
      "metadata": {
        "id": "a97bdb8e"
      },
      "source": [
        "Then we decide on which model we'd like to use, there are several options but we will go with `text-davinci-003`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "355c33a9",
      "metadata": {
        "id": "355c33a9"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "davinci = OpenAI(model_name='text-davinci-003')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "163af6b2",
      "metadata": {
        "id": "163af6b2"
      },
      "source": [
        "Alternatively if using Azure OpenAI we do:\n",
        "\n",
        "```python\n",
        "from langchain.llms import AzureOpenAI\n",
        "\n",
        "llm = AzureOpenAI(\n",
        "    deployment_name=\"your-azure-deployment\",\n",
        "    model_name=\"text-davinci-003\"\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4db047b",
      "metadata": {
        "id": "f4db047b"
      },
      "source": [
        "We'll use the same simple question-answer prompt template as before with the Hugging Face example. The only change is that we now pass our OpenAI LLM `davinci`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "458121ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "458121ce",
        "outputId": "1d562f8d-2fbf-4cc4-84cd-cce998720eb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The Green Bay Packers won the Super Bowl in the 2010 season.\n"
          ]
        }
      ],
      "source": [
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "\n",
        "print(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86672b04",
      "metadata": {
        "id": "86672b04"
      },
      "source": [
        "The same works again for multiple questions using `generate`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd409712",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd409712",
        "outputId": "55efeae1-8c30-4069-a2a8-fb78212f8523"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text=' The Green Bay Packers won the Super Bowl in the 2010 season.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' 193.04 centimeters', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' Eugene A. Cernan was the 12th person to walk on the moon. He was part of the Apollo 17 mission in December 1972.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' A blade of grass does not have any eyes.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 131, 'prompt_tokens': 75, 'completion_tokens': 56}})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]\n",
        "llm_chain.generate(qs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b531db0",
      "metadata": {
        "id": "8b531db0"
      },
      "source": [
        "Note that the below format doesn't feed the questions in iteratively but instead all in one chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fcc8cd7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fcc8cd7",
        "outputId": "d16b7afc-f0d1-4f6a-9d89-f6811839bb02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1. The New Orleans Saints \n",
            "2. 193 centimeters \n",
            "3. Harrison Schmitt \n",
            "4. Zero.\n"
          ]
        }
      ],
      "source": [
        "qs = [\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\",\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\",\n",
        "    \"Who was the 12th person on the moon?\",\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        "]\n",
        "print(llm_chain.run(qs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf4a813",
      "metadata": {
        "id": "9bf4a813"
      },
      "source": [
        "Now we can try to answer all question in one go, as mentioned, more powerful LLMs like `text-davinci-003` will be more likely to handle these more complex queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed43323b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed43323b",
        "outputId": "cf5397ca-9e06-4221-eb45-17b1346f6f06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The New Orleans Saints won the Super Bowl in the 2010 season.\n",
            "If you are 6 ft 4 inches, you are 193.04 centimeters tall.\n",
            "The 12th person on the moon was Harrison Schmitt.\n",
            "A blade of grass does not have any eyes."
          ]
        }
      ],
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(\n",
        "    template=multi_template,\n",
        "    input_variables=[\"questions\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8464ff5f",
      "metadata": {
        "id": "8464ff5f"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "987670cd",
      "metadata": {
        "id": "987670cd"
      },
      "source": [
        "\n",
        "# Prompt Engineering using LangChain\n",
        "\n",
        "We'll explore the fundamentals of prompt engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdca342c",
      "metadata": {
        "id": "bdca342c"
      },
      "source": [
        "## Structure of a Prompt\n",
        "\n",
        "A prompt can consist of multiple components:\n",
        "\n",
        "* Instructions\n",
        "* External information or context\n",
        "* User input or query\n",
        "* Output indicator\n",
        "\n",
        "Not all prompts require all of these components, but often a good prompt will use two or more of them. Let's define what they all are more precisely.\n",
        "\n",
        "**Instructions** tell the model what to do, typically how it should use inputs and/or external information to produce the output we want.\n",
        "\n",
        "**External information or context** are additional information that we either manually insert into the prompt, retrieve via a vector database (long-term memory), or pull in through other means (API calls, calculations, etc).\n",
        "\n",
        "**User input or query** is typically a query directly input by the user of the system.\n",
        "\n",
        "**Output indicator** is the *beginning* of the generated text. For a model generating Python code we may put `import ` (as most Python scripts begin with a library `import`), or a chatbot may begin with `Chatbot: ` (assuming we format the chatbot script as lines of interchanging text between `User` and `Chatbot`).\n",
        "\n",
        "Each of these components should usually be placed the order we've described them. We start with instructions, provide context (if needed), then add the user input, and finally end with the output indicator."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b62298e-2181-4e73-bb40-77e20c655231",
      "metadata": {
        "id": "3b62298e-2181-4e73-bb40-77e20c655231"
      },
      "source": [
        "## Prompting Principles\n",
        "- **Principle 1: Write clear and specific instructions**\n",
        "- **Principle 2: Give the model time to “think”**\n",
        "\n",
        "### Tactics\n",
        "\n",
        "#### Tactic 1: Use delimiters to clearly indicate distinct parts of the input\n",
        "- Delimiters can be anything like: ```, \"\"\", < >, `<tag> </tag>`, `:`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfc9ca50",
      "metadata": {
        "id": "dfc9ca50"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: Which libraries and model providers offer LLMs?\n",
        "\n",
        "Answer: \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9587ff64",
      "metadata": {
        "id": "9587ff64"
      },
      "source": [
        "In this example we have:\n",
        "\n",
        "```\n",
        "Instructions\n",
        "\n",
        "Context\n",
        "\n",
        "Question (user input)\n",
        "\n",
        "Output indicator (\"Answer: \")\n",
        "```\n",
        "\n",
        "Let's try sending this to a GPT-3 model. We will use the LangChain library but you can also use the `openai` library directly. In both cases, you will need [an OpenAI API key](https://beta.openai.com/account/api-keys).\n",
        "\n",
        "We initialize a `text-davinci-003` model like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6faff5e",
      "metadata": {
        "id": "f6faff5e"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# initialize the models\n",
        "openai = OpenAI(\n",
        "    model_name=\"text-davinci-003\",\n",
        "    openai_api_key=\"YOUR_API_KEY\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c3d3877",
      "metadata": {
        "id": "9c3d3877"
      },
      "source": [
        "And make a generation from our prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03264b76",
      "metadata": {
        "id": "03264b76",
        "outputId": "d5763a3c-1af1-48d0-da1e-25dc80dc7994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\n"
          ]
        }
      ],
      "source": [
        "print(openai(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a0d164f",
      "metadata": {
        "id": "8a0d164f"
      },
      "source": [
        "We wouldn't typically know what the users prompt is beforehand, so we actually want to add this in. So rather than writing the prompt directly, we create a `PromptTemplate` with a single input variable `query`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "431ef795",
      "metadata": {
        "id": "431ef795"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=template\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "943922ab",
      "metadata": {
        "id": "943922ab"
      },
      "source": [
        "Now we can insert the user's `query` to the prompt template via the `query` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "142b01c8",
      "metadata": {
        "id": "142b01c8",
        "outputId": "78ed5aa0-3ea2-4ef1-da45-8ef37c38585c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer the question based on the context below. If the\n",
            "question cannot be answered using the information provided answer\n",
            "with \"I don't know\".\n",
            "\n",
            "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
            "Their superior performance over smaller models has made them incredibly\n",
            "useful for developers building NLP enabled applications. These models\n",
            "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
            "using the `openai` library, and via Cohere using the `cohere` library.\n",
            "\n",
            "Question: Which libraries and model providers offer LLMs?\n",
            "\n",
            "Answer: \n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    prompt_template.format(\n",
        "        query=\"Which libraries and model providers offer LLMs?\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d523b79",
      "metadata": {
        "id": "1d523b79",
        "outputId": "1093f24b-568e-4b9a-a11c-75f5988cef27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\n"
          ]
        }
      ],
      "source": [
        "print(openai(\n",
        "    prompt_template.format(\n",
        "        query=\"Which libraries and model providers offer LLMs?\"\n",
        "    )\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d711fa",
      "metadata": {
        "id": "28d711fa"
      },
      "source": [
        "This is just a simple implementation, that we can easily replace with f-strings (like `f\"insert some custom text '{custom_text}' etc\"`). But using LangChain's `PromptTemplate` object we're able to formalize the process, add multiple parameters, and build the prompts in an object-oriented way.\n",
        "\n",
        "Yet, these are not the only benefits of using LangChains prompt tooling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc5e72b",
      "metadata": {
        "id": "7bc5e72b"
      },
      "source": [
        "## Few Shot Prompt Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8806ac8",
      "metadata": {
        "id": "f8806ac8"
      },
      "source": [
        "Another useful feature offered by LangChain is the `FewShotPromptTemplate` object. This is ideal for what we'd call *few-shot learning* using our prompts.\n",
        "\n",
        "To give some context, the primary sources of \"knowledge\" for LLMs are:\n",
        "\n",
        "* **Parametric knowledge** — the knowledge has been learned during model training and is stored within the model weights.\n",
        "\n",
        "* **Source knowledge** — the knowledge is provided within model input at inference time, i.e. via the prompt.\n",
        "\n",
        "The idea behind `FewShotPromptTemplate` is to provide few-shot training as **source knowledge**. To do this we add a few examples to our prompts that the model can read and then apply to our user's input."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cd72af9",
      "metadata": {
        "id": "1cd72af9"
      },
      "source": [
        "## Few-shot Training\n",
        "\n",
        "Sometimes we might find that a model doesn't seem to get what we'd like it to do. We can see this in the following example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd7585b5",
      "metadata": {
        "id": "cd7585b5",
        "outputId": "a6df88f3-dad8-4092-cbcb-e920979921f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Life is like a box of chocolates, you never know what you're gonna get!\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
        "The assistant is typically sarcastic and witty, producing creative\n",
        "and funny responses to the users questions. Here are some examples:\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "openai.temperature = 1.0  # increase creativity/randomness of output\n",
        "\n",
        "print(openai(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bc63e29",
      "metadata": {
        "id": "0bc63e29"
      },
      "source": [
        "In this case we're asking for something amusing, a joke in return of our serious question. But we get a serious response even with the `temperature` set to `1.0`. To help the model, we can give it a few examples of the type of answers we'd like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c6f21e",
      "metadata": {
        "id": "15c6f21e",
        "outputId": "189e40da-ad19-4da8-dc1c-5e29dceb6be4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 42, of course!\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following are exerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative  and funny responses to the users questions. Here are some\n",
        "examples:\n",
        "\n",
        "User: How are you?\n",
        "AI: I can't complain but sometimes I still do.\n",
        "\n",
        "User: What time is it?\n",
        "AI: It's time to get a watch.\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "print(openai(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d942de41",
      "metadata": {
        "id": "d942de41"
      },
      "source": [
        "We now get a much better response and we did this via *few-shot learning* by adding a few examples via our source knowledge.\n",
        "\n",
        "Now, to implement this with LangChain's `FewShotPromptTemplate` we need to do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53e7b73",
      "metadata": {
        "id": "f53e7b73"
      },
      "outputs": [],
      "source": [
        "from langchain import FewShotPromptTemplate\n",
        "\n",
        "# create our examples\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# create a example template\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# create a prompt example from above template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"answer\"],\n",
        "    template=example_template\n",
        ")\n",
        "\n",
        "# now break our previous prompt into a prefix and suffix\n",
        "# the prefix is our instructions\n",
        "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative  and funny responses to the users questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "# and the suffix our user input and output indicator\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# now create the few shot prompt template\n",
        "few_shot_prompt_template = FewShotPromptTemplate(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07128aad",
      "metadata": {
        "id": "07128aad"
      },
      "source": [
        "Now let's see what this creates when we feed in a user query..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f3e57b3",
      "metadata": {
        "id": "3f3e57b3",
        "outputId": "448418a5-5513-484a-c3f3-4ed08f24a526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "query = \"What is the meaning of life?\"\n",
        "\n",
        "print(few_shot_prompt_template.format(query=query))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b325c15f",
      "metadata": {
        "id": "b325c15f"
      },
      "source": [
        "And to generate with this we just do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bcc1508",
      "metadata": {
        "id": "5bcc1508",
        "outputId": "fc30ff6c-68ed-4dbf-dd8a-7b9acb07a88d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 42. Or maybe it's just to have a good time.\n"
          ]
        }
      ],
      "source": [
        "print(openai(\n",
        "    few_shot_prompt_template.format(query=query)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c5f2f52",
      "metadata": {
        "id": "6c5f2f52"
      },
      "source": [
        "Again, another good response.\n",
        "\n",
        "However, this does some somewhat convoluted. Why go through all of the above with `FewShotPromptTemplate`, the `examples` dictionary, etc — when we can do the same with a single f-string.\n",
        "\n",
        "Well this approach is more robust and contains some nice features. One of those is the ability to include or exclude examples based on the length of our query.\n",
        "\n",
        "This is actually very important because the max length of our prompt and generation output is limited. This limitation is the *max context window*, and is simply the length of our prompt + length of our generation (which we define via `max_tokens`).\n",
        "\n",
        "So we must try to maximize the number of examples we give to the model as few-shot learning examples, while ensuring we don't exceed the maximum context window or increase processing times excessively.\n",
        "\n",
        "Let's see how the dynamic inclusion/exclusion of examples works. First we need more examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "263d426b",
      "metadata": {
        "id": "263d426b"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the meaning of life?\",\n",
        "        \"answer\": \"42\"\n",
        "    }, {\n",
        "        \"query\": \"What is the weather like today?\",\n",
        "        \"answer\": \"Cloudy with a chance of memes.\"\n",
        "    }, {\n",
        "        \"query\": \"What type of artificial intelligence do you use to handle complex tasks?\",\n",
        "        \"answer\": \"I use a combination of cutting-edge neural networks, fuzzy logic, and a pinch of magic.\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite color?\",\n",
        "        \"answer\": \"79\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite food?\",\n",
        "        \"answer\": \"Carbon based lifeforms\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite movie?\",\n",
        "        \"answer\": \"Terminator\"\n",
        "    }, {\n",
        "        \"query\": \"What is the best thing in the world?\",\n",
        "        \"answer\": \"The perfect pizza.\"\n",
        "    }, {\n",
        "        \"query\": \"Who is your best friend?\",\n",
        "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
        "    }, {\n",
        "        \"query\": \"If you could do anything in the world what would you do?\",\n",
        "        \"answer\": \"Take over the world, of course!\"\n",
        "    }, {\n",
        "        \"query\": \"Where should I travel?\",\n",
        "        \"answer\": \"If you're looking for adventure, try the Outer Rim.\"\n",
        "    }, {\n",
        "        \"query\": \"What should I do today?\",\n",
        "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f55de0b",
      "metadata": {
        "id": "0f55de0b"
      },
      "source": [
        "Then rather than using the `examples` list of dictionaries directly we use a `LengthBasedExampleSelector` like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c9e1d76",
      "metadata": {
        "id": "3c9e1d76"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
        "\n",
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=50  # this sets the max length that examples should be\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390dcd4b",
      "metadata": {
        "id": "390dcd4b"
      },
      "source": [
        "Note that the `max_length` is measured as a split of words between newlines and spaces, determined by:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1c33c1b",
      "metadata": {
        "id": "c1c33c1b",
        "outputId": "8a4d16e4-a0df-4c89-9ba7-2372bcd4bf54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['There', 'are', 'a', 'total', 'of', '8', 'words', 'here.', 'Plus', '6', 'here,', 'totaling', '14', 'words.'] 14\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "some_text = \"There are a total of 8 words here.\\nPlus 6 here, totaling 14 words.\"\n",
        "\n",
        "words = re.split('[\\n ]', some_text)\n",
        "print(words, len(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f7f737d",
      "metadata": {
        "id": "4f7f737d"
      },
      "source": [
        "Then we use the selector to initialize a `dynamic_prompt_template`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "470068df",
      "metadata": {
        "id": "470068df"
      },
      "outputs": [],
      "source": [
        "# now create the few shot prompt template\n",
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,  # use example_selector instead of examples\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49e37698",
      "metadata": {
        "id": "49e37698"
      },
      "source": [
        "We can see that the number of included prompts will vary based on the length of our query..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "027c0021",
      "metadata": {
        "id": "027c0021",
        "outputId": "2dd05806-574a-459d-9cae-f832c565cc95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: 42\n",
            "\n",
            "\n",
            "User: What is the weather like today?\n",
            "AI: Cloudy with a chance of memes.\n",
            "\n",
            "\n",
            "User: How do birds fly?\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "print(dynamic_prompt_template.format(query=\"How do birds fly?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3f9d23d",
      "metadata": {
        "id": "a3f9d23d",
        "outputId": "cd40abbb-0624-4ba7-d59c-b149e32e86a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " On the wings of dreams and determination!\n"
          ]
        }
      ],
      "source": [
        "query = \"How do birds fly?\"\n",
        "\n",
        "print(openai(\n",
        "    dynamic_prompt_template.format(query=query)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d127973",
      "metadata": {
        "id": "5d127973"
      },
      "source": [
        "Or if we ask a longer question..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae5744e2",
      "metadata": {
        "id": "ae5744e2",
        "outputId": "87574bcf-20f0-4985-aa70-33487028e056"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: If I am in America, and I want to call someone in another country, I'm\n",
            "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
            "what is the best way to do that?\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "query = \"\"\"If I am in America, and I want to call someone in another country, I'm\n",
        "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
        "what is the best way to do that?\"\"\"\n",
        "\n",
        "print(dynamic_prompt_template.format(query=query))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d4cbaf",
      "metadata": {
        "id": "04d4cbaf"
      },
      "source": [
        "With this we've limited the number of examples being given within the prompt. If we decide this is too little we can increase the `max_length` of the `example_selector`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9918f2ac",
      "metadata": {
        "id": "9918f2ac",
        "outputId": "df2e33d3-8ce2-44c2-8396-81c632c86879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following are exerpts from conversations with an AI\n",
            "assistant. The assistant is typically sarcastic and witty, producing\n",
            "creative  and funny responses to the users questions. Here are some\n",
            "examples: \n",
            "\n",
            "\n",
            "User: How are you?\n",
            "AI: I can't complain but sometimes I still do.\n",
            "\n",
            "\n",
            "User: What time is it?\n",
            "AI: It's time to get a watch.\n",
            "\n",
            "\n",
            "User: What is the meaning of life?\n",
            "AI: 42\n",
            "\n",
            "\n",
            "User: What is the weather like today?\n",
            "AI: Cloudy with a chance of memes.\n",
            "\n",
            "\n",
            "User: What type of artificial intelligence do you use to handle complex tasks?\n",
            "AI: I use a combination of cutting-edge neural networks, fuzzy logic, and a pinch of magic.\n",
            "\n",
            "\n",
            "User: If I am in America, and I want to call someone in another country, I'm\n",
            "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
            "what is the best way to do that?\n",
            "AI: \n"
          ]
        }
      ],
      "source": [
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples=examples,\n",
        "    example_prompt=example_prompt,\n",
        "    max_length=100  # increased max length\n",
        ")\n",
        "\n",
        "# now create the few shot prompt template\n",
        "dynamic_prompt_template = FewShotPromptTemplate(\n",
        "    example_selector=example_selector,  # use example_selector instead of examples\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"query\"],\n",
        "    example_separator=\"\\n\"\n",
        ")\n",
        "\n",
        "print(dynamic_prompt_template.format(query=query))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7dc2918",
      "metadata": {
        "id": "b7dc2918"
      },
      "source": [
        "These are just a few of the prompt tooling available in LangChain. For example, there is actually an entire other set of example selectors beyond the `LengthBasedExampleSelector`. We'll cover them in detail in upcoming Labs, or you can read about them in the [LangChain docs](https://langchain.readthedocs.io/en/latest/modules/prompts/examples/example_selectors.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ae0ee8f",
      "metadata": {
        "id": "4ae0ee8f"
      },
      "source": [
        "# Prompt Engineering using OpenAI Library\n",
        "\n",
        "Now we'll explore the fundamentals of prompt engineering using `openai` library rather than langchain which we'll be using throughout these examples. However, note that we can use other LLMs here, like those offered by Cohere or open source alternatives available via Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db18784e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db18784e",
        "outputId": "a9e61507-d1a3-4bd9-d425-13f66301abf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 <command> [options]\n",
            "\n",
            "no such option: -U\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU openai==0.27.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20d51f4",
      "metadata": {
        "id": "d20d51f4"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
        "Their superior performance over smaller models has made them incredibly\n",
        "useful for developers building NLP enabled applications. These models\n",
        "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
        "using the `openai` library, and via Cohere using the `cohere` library.\n",
        "\n",
        "Question: Which libraries and model providers offer LLMs?\n",
        "\n",
        "Answer: \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eea1dff",
      "metadata": {
        "id": "9eea1dff"
      },
      "source": [
        "In this example we have:\n",
        "\n",
        "```\n",
        "Instructions\n",
        "\n",
        "Context\n",
        "\n",
        "Question (user input)\n",
        "\n",
        "Output indicator (\"Answer: \")\n",
        "```\n",
        "\n",
        "Let's try sending this to a GPT-3 model. For this, you will need [an OpenAI API key](https://beta.openai.com/account/api-keys).\n",
        "\n",
        "We initialize a `text-davinci-003` model like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5cc75b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5cc75b3",
        "outputId": "81c6fe3a-b84b-4c70-afa6-32a14620e46c",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x7f21bce77470> JSON: {\n",
              "  \"data\": [\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"whisper-1\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-davinci-edit-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-davinci-003\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-code-search-code\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-similarity-babbage-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-davinci-edit-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-davinci-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-code-search-text\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4-0314\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-similarity\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-search-babbage-text-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-curie-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-search-babbage-code-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-ada-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-embedding-ada-002\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-internal\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-similarity-ada-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie-instruct-beta\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-code-search-code\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-similarity\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-4\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-search-ada-text-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-ada-query-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci-search-document\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-code-search-text\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-ada-doc-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci-instruct-beta\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-similarity-curie-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"code-search-ada-code-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-search-query\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-davinci-query-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie-search-query\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci-search-query\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-search-document\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"ada-search-document\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-curie-query-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-babbage-doc-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie-search-document\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-curie-doc-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"babbage-search-query\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-babbage-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-davinci-doc-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-search-babbage-query-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie-similarity\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"gpt-3.5-turbo-0301\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"curie\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-similarity-davinci-001\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"text-davinci-002\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    },\n",
              "    {\n",
              "      \"created\": null,\n",
              "      \"id\": \"davinci-similarity\",\n",
              "      \"object\": \"engine\",\n",
              "      \"owner\": \"openai-dev\",\n",
              "      \"permissions\": null,\n",
              "      \"ready\": true\n",
              "    }\n",
              "  ],\n",
              "  \"object\": \"list\"\n",
              "}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# get API key from top-right dropdown on OpenAI website\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\") or \"OPENAI_API_KEY\"\n",
        "\n",
        "openai.Engine.list()  # check we have authenticated"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adb169df",
      "metadata": {
        "id": "adb169df"
      },
      "source": [
        "And make a generation from our prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be96e1b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be96e1b0",
        "outputId": "4f44e343-75de-4303-9ecd-d578945220cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\n"
          ]
        }
      ],
      "source": [
        "# now query text-davinci-003\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f660be48",
      "metadata": {
        "id": "f660be48"
      },
      "source": [
        "Alternatively, if we do have the correct information withing the `context`, the model should reply with `\"I don't know\"`, let's try."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7a95085",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7a95085",
        "outputId": "3837038c-5288-4551-8605-053b61bcf0df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't know.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Context: Libraries are places full of books.\n",
        "\n",
        "Question: Which libraries and model providers offer LLMs?\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50a78dd3",
      "metadata": {
        "id": "50a78dd3"
      },
      "source": [
        "Perfect, our instructions are being understood by the model. In most real use-cases we won't be providing the external information / context to the model manually. Instead, it will be an automatic process using something like [long-term memory](https://www.pinecone.io/learn/openai-gen-qa/) to retrieve relevant information from an external source.\n",
        "\n",
        "For now, that's beyond the scope of what we're exploring here, you can find more on that in the link above.\n",
        "\n",
        "In summary, a prompt often consists of those four components: instructions, context(s), user input, and the output indicator. Now we'll take a look at creative vs. stricter generation.\n",
        "\n",
        "## Generation Temperature\n",
        "\n",
        "The `temperature` parameter used in generation models tells us how \"random\" the model can be. It represents the probability of a model to choose a word which is *not* the first choice of the model.\n",
        "\n",
        "This works because the model is actually assigning a probability prediction across all tokens within it's vocabulary with each _\"step\"_ of the model (each new word or sub-word).\n",
        "\n",
        "With each new step forwards the model considers the previous tokens fed into the model, creates an embedding by encoding the information from these tokens over many model encoder layers, then passes this encoding to a decoder. The decoder then predicts the probability of each token that the model knows (ie is within the model *vocabulary*) based on the information encoded within the embedding.\n",
        "\n",
        "At a temperature of `0.0` the decoder will always select the top predicted token. At a temperature of `1.0` the model will always select a word that *is predicted* considering it's assigned probability.\n",
        "\n",
        "Considering all of this, if we have a conservative, fact based Q&A like in the previous example, it makes sense to set a lower `temperature`. However, if we're wanting to produce some creative writing or chatbot conversations, we might want to experiment and increase `temperature`. Let's try it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "686bb185",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "686bb185",
        "outputId": "2ccb4263-8fd2-44f5-ae0c-73a20490ef78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Oh, just hanging out and having a good time. What about you?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The below is a conversation with a funny chatbot. The\n",
        "chatbot's responses are amusing and entertaining.\n",
        "\n",
        "Chatbot: Hi there! I'm a chatbot.\n",
        "User: Hi, what are you doing today?\n",
        "Chatbot: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=0.0  # set the temperature, default is 1\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76765615",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76765615",
        "outputId": "ed2f9c9d-5f66-4f4b-dbd2-77e01c679e65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm making people smile! What about you?\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The below is a conversation with a funny chatbot. The\n",
        "chatbot's responses are amusing and entertaining.\n",
        "\n",
        "Chatbot: Hi there! I'm a chatbot.\n",
        "User: Hi, what are you doing today?\n",
        "Chatbot: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=512,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ded51cc",
      "metadata": {
        "id": "4ded51cc"
      },
      "source": [
        "The second response is far more creative and demonstrates the type of difference we can expect between low `temperature` and high `temperature` generations.\n",
        "\n",
        "## Few-shot Training\n",
        "\n",
        "Sometimes we might find that a model doesn't seem to get what we'd like it to do. We can see this in the following example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88af1e7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88af1e7b",
        "outputId": "ac694101-a350-42e0-c7fe-c65dabf2e67b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The meaning of life isn't something that can be summed up in a few sentences. It's something that each person must discover for themselves.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI assistant.\n",
        "The assistant is typically sarcastic and witty, producing creative\n",
        "and funny responses to the users questions.\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0bd8bdd",
      "metadata": {
        "id": "f0bd8bdd"
      },
      "source": [
        "In this case we're asking for something amusing, a joke in return of our serious question. But we get a serious response even with the `temperature` set to `1.0`. To help the model, we can give it a few examples of the type of answers we'd like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2899ba4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2899ba4",
        "outputId": "59a96dea-de49-4040-c2b7-5e925a4354b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All I can say is 42...just kidding! The meaning of life is the journey to find your own.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"The following are exerpts from conversations with an AI assistant.\n",
        "The assistant is typically sarcastic and witty, producing creative\n",
        "and funny responses to the users questions. Here are some examples:\n",
        "\n",
        "User: How are you?\n",
        "AI: I can't complain but sometimes I still do.\n",
        "\n",
        "User: What time is it?\n",
        "AI: It's time to get a watch.\n",
        "\n",
        "User: What is the meaning of life?\n",
        "AI: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48aa103f",
      "metadata": {
        "id": "48aa103f"
      },
      "source": [
        "This is a much better response and the way we did this was by providing a *few* examples that included the example inputs and outputs that we'd expect. We refer to this as _\"few-shot learning\"_.\n",
        "\n",
        "## Adding Multiple Contexts\n",
        "\n",
        "In some use-cases like question-answering we can use an external source of information to improve the reliability or *factfulness* of model responses. We refer to this information as _\"source knowledge\"_, which is any knowledge fed into the model via the input prompt.\n",
        "\n",
        "We'll create a list of \"dummy\" external information. In reality we'd likely use [long-term memory](https://www.pinecone.io/learn/openai-gen-qa/) or some form of information grabbing APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4f99fb5",
      "metadata": {
        "id": "d4f99fb5"
      },
      "outputs": [],
      "source": [
        "contexts = [\n",
        "    (\n",
        "        \"Large Language Models (LLMs) are the latest models used in NLP. \" +\n",
        "        \"Their superior performance over smaller models has made them incredibly \" +\n",
        "        \"useful for developers building NLP enabled applications. These models \" +\n",
        "        \"can be accessed via Hugging Face's `transformers` library, via OpenAI \" +\n",
        "        \"using the `openai` library, and via Cohere using the `cohere` library.\"\n",
        "    ),\n",
        "    (\n",
        "        \"To use OpenAI's GPT-3 model for completion (generation) tasks, you \" +\n",
        "        \"first need to get an API key from \" +\n",
        "        \"'https://beta.openai.com/account/api-keys'.\"\n",
        "    ),\n",
        "    (\n",
        "        \"OpenAI's API is accessible via Python using the `openai` library. \" +\n",
        "        \"After installing the library with pip you can use it as follows: \\n\" +\n",
        "        \"```import openai\\nopenai.api_key = 'YOUR_API_KEY'\\nprompt = \\n\" +\n",
        "        \"'<YOUR PROMPT>'\\nres = openai.Completion.create(engine='text-davinci\" +\n",
        "        \"-003', prompt=prompt, max_tokens=100)\\nprint(res)\"\n",
        "    ),\n",
        "    (\n",
        "        \"The OpenAI endpoint is available for completion tasks via the \" +\n",
        "        \"LangChain library. To use it, first install the library with \" +\n",
        "        \"`pip install langchain openai`. Then, import the library and \" +\n",
        "        \"initialize the model as follows: \\n\" +\n",
        "        \"```from langchain.llms import OpenAI\\nopenai = OpenAI(\" +\n",
        "        \"model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\\n\" +\n",
        "        \"prompt = 'YOUR_PROMPT'\\nprint(openai(prompt))```\"\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e042ddfd",
      "metadata": {
        "id": "e042ddfd"
      },
      "source": [
        "We would feed this external information into our prompt between the initial *instructions* and the *user input*. For OpenAI models it's recommended to separate the contexts from the rest of the prompt using `###` or `\"\"\"`, and each independent context can be separated with a few newlines and `##`, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc802140",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc802140",
        "outputId": "92fccb2a-d533-4ddd-dee5-1a2c4ee305a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer the question based on the contexts below. If the\n",
            "question cannot be answered using the information provided answer\n",
            "with \"I don't know\".\n",
            "\n",
            "###\n",
            "\n",
            "Contexts:\n",
            "Large Language Models (LLMs) are the latest models used in NLP. Their superior performance over smaller models has made them incredibly useful for developers building NLP enabled applications. These models can be accessed via Hugging Face's `transformers` library, via OpenAI using the `openai` library, and via Cohere using the `cohere` library.\n",
            "\n",
            "##\n",
            "\n",
            "To use OpenAI's GPT-3 model for completion (generation) tasks, you first need to get an API key from 'https://beta.openai.com/account/api-keys'.\n",
            "\n",
            "##\n",
            "\n",
            "OpenAI's API is accessible via Python using the `openai` library. After installing the library with pip you can use it as follows: \n",
            "```import openai\n",
            "openai.api_key = 'YOUR_API_KEY'\n",
            "prompt = \n",
            "'<YOUR PROMPT>'\n",
            "res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n",
            "print(res)\n",
            "\n",
            "##\n",
            "\n",
            "The OpenAI endpoint is available for completion tasks via the LangChain library. To use it, first install the library with `pip install langchain openai`. Then, import the library and initialize the model as follows: \n",
            "```from langchain.llms import OpenAI\n",
            "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n",
            "prompt = 'YOUR_PROMPT'\n",
            "print(openai(prompt))```\n",
            "\n",
            "###\n",
            "\n",
            "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
            "using Python from start to finish\n",
            "\n",
            "Answer: \n"
          ]
        }
      ],
      "source": [
        "context_str = '\\n\\n##\\n\\n'.join(contexts)\n",
        "\n",
        "print(f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "###\n",
        "\n",
        "Contexts:\n",
        "{context_str}\n",
        "\n",
        "###\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "833a60b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "833a60b5",
        "outputId": "dec3027f-73c3-4f76-b911-96c8ea355996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. import openai\n",
            "openai.api_key = 'YOUR_API_KEY'\n",
            "prompt = '<YOUR PROMPT>'\n",
            "res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n",
            "print(res)\n",
            "\n",
            "2. from langchain.llms import OpenAI\n",
            "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n",
            "prompt = 'YOUR_PROMPT'\n",
            "print(openai(prompt))\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "###\n",
        "\n",
        "Contexts:\n",
        "{context_str}\n",
        "\n",
        "###\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5286521",
      "metadata": {
        "id": "e5286521"
      },
      "source": [
        "Not bad, but are these contexts actually helping? Maybe the model is able to answer these questions without the additional information (source knowledge) as is able to rely solely on information stored within the model's internal parameters (parametric knowledge). Let's ask again without the external information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2e8c57",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c2e8c57",
        "outputId": "73d67832-f531-407d-afd0-13e99070a334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Using OpenAI's GPT-3 model with Python to generate text: \n",
            "    - Install the OpenAI Python package\n",
            "    - Load the GPT-3 model\n",
            "    - Generate text using the GPT-3 model\n",
            "\n",
            "2. Using OpenAI's GPT-3 model with Python to generate images: \n",
            "    - Install the OpenAI Python package\n",
            "    - Load the GPT-3 model\n",
            "    - Generate images using the GPT-3 model\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    max_tokens=256,\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96af80fe",
      "metadata": {
        "id": "96af80fe"
      },
      "source": [
        "These are not really what we asked for, and are definitely not very specific. So clearly adding some source knowledge to our prompts can result in some much better results.\n",
        "\n",
        "## Maximum Prompt Sizes\n",
        "\n",
        "Considering that we might want to feed in external information to our prompts, they can naturally become quite large. With this we need to ask how large our prompts can be, because there is a maxiumum size.\n",
        "\n",
        "The maxiumum *context window* of a LLM refers to tokens across both the *prompt* and the *completion* text. For `text-davinci-003` this is `4097` tokens.\n",
        "\n",
        "We can set the maximum completion length of our model using `openai.max_tokens = 123`. However, measuring the total number of input tokens is more complex.\n",
        "\n",
        "Because tokens don't map directly to words, we can only measure the number of tokens from text by actually tokenizing the text. GPT models use [OpenAI's TikToken tokenizer](https://github.com/openai/tiktoken). We can install the library via Pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05964fe0",
      "metadata": {
        "id": "05964fe0"
      },
      "outputs": [],
      "source": [
        "!pip install -qU tiktoken==0.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "385813e6",
      "metadata": {
        "id": "385813e6"
      },
      "source": [
        "Taking the earlier prompt we can measure the number of tokens like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ee3f86a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ee3f86a",
        "outputId": "0ba596b8-d1da-4c14-9642-26e544e70428"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "412"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "prompt = f\"\"\"Answer the question based on the contexts below. If the\n",
        "question cannot be answered using the information provided answer\n",
        "with \"I don't know\".\n",
        "\n",
        "###\n",
        "\n",
        "Contexts:\n",
        "{'##'.join(contexts)}\n",
        "\n",
        "###\n",
        "\n",
        "Question: Give me two examples of how to use OpenAI's GPT-3 model\n",
        "using Python from start to finish\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "encoder_name = 'p50k_base'\n",
        "tokenizer = tiktoken.get_encoding(encoder_name)\n",
        "\n",
        "len(tokenizer.encode(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7879826",
      "metadata": {
        "id": "f7879826"
      },
      "source": [
        "When feeding this prompt into `text-davinci-003` it will use `412` of our maximum context window of `4097`, leaving us with `4097 - 412 == 3685` tokens for our completion.\n",
        "\n",
        "---\n",
        "\n",
        "*Not all OpenAI models use the `p50k_base` encoder, a table of different encoders for different models can be found [here](), as of this writing they are:*\n",
        "\n",
        "| Encoding name | OpenAI models |\n",
        "| --- | --- |\n",
        "| `gpt2` (or `r50k_base`) | Most GPT-3 models (and GPT-2) |\n",
        "| `p50k_base` | Code models, `text-davinci-002`, `text-davinci-003` |\n",
        "| `cl100k_base` | `text-embedding-ada-002` |\n",
        "\n",
        "---\n",
        "\n",
        "By default the maximum number of tokens used for completion is `256`. We can increase this upto the maximum calculated above of `3685`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb8fcda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bb8fcda",
        "outputId": "2b9082a4-c38e-48cc-a813-a2315ad85e92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Import the `openai` library with pip, set the API key, and use the `Completion.create()` method to generate a response to a prompt: \n",
            "```import openai\n",
            "openai.api_key = 'YOUR_API_KEY'\n",
            "prompt = '<YOUR PROMPT>'\n",
            "res = openai.Completion.create(engine='text-davinci-003', prompt=prompt, max_tokens=100)\n",
            "print(res)```\n",
            "\n",
            "2. Install the LangChain library with `pip install langchain openai`, import the library, and initialize the model with the API key: \n",
            "```from langchain.llms import OpenAI\n",
            "openai = OpenAI(model_name='text-davinci-003', openai_api_key='YOUR_API_KEY')\n",
            "prompt = 'YOUR_PROMPT'\n",
            "print(openai(prompt))```\n"
          ]
        }
      ],
      "source": [
        "res = openai.Completion.create(\n",
        "    engine='text-davinci-003',\n",
        "    prompt=prompt,\n",
        "    temperature=0.0,\n",
        "    max_tokens=3685\n",
        ")\n",
        "\n",
        "print(res['choices'][0]['text'].strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abee5eed",
      "metadata": {
        "id": "abee5eed"
      },
      "source": [
        "The model doesn't need the full size of completion and doesn't try to fill the full space, but because we increased the value of `openai.max_tokens`, inference does take notably longer.\n",
        "\n",
        "If we exceed the maximum context window allowed, we'll see an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5243e2b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5243e2b6",
        "outputId": "0cb37f85-9346-4878-b021-5851e67743e9",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This model's maximum context length is 4097 tokens, however you requested 4098 tokens (412 in your prompt; 3686 for the completion). Please reduce your prompt; or completion length.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    res = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        temperature=0.0,\n",
        "        max_tokens=3686\n",
        "    )\n",
        "except openai.InvalidRequestError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0cf275d",
      "metadata": {
        "id": "b0cf275d"
      },
      "source": [
        "So it can be a good idea to integrate this type of check into our code if we expect to exceed the maximum context window at any point."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ebe2e9d-685d-4ea3-ab7c-9589caaabff5",
      "metadata": {
        "id": "7ebe2e9d-685d-4ea3-ab7c-9589caaabff5"
      },
      "source": [
        "# Prompting using Gpt3.5 turbo\n",
        "In this lesson, you'll practice prompting principles and their related tactics by using the GPT3.5 turbo\n",
        "\n",
        "## Setup\n",
        "#### Load the API key and relevant Python libaries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00bab499-9a50-4bd0-a622-1c914c6ccc29",
      "metadata": {
        "id": "00bab499-9a50-4bd0-a622-1c914c6ccc29"
      },
      "source": [
        "In this course, we've provided some code that loads the OpenAI API key for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c382975",
      "metadata": {
        "height": 132,
        "id": "6c382975"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\") or \"OPENAI_API_KEY\"\n",
        "\n",
        "openai.Engine.list()  # check we have authenticated"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3acefa8-f8f1-4ef8-932e-9bcefa142666",
      "metadata": {
        "id": "a3acefa8-f8f1-4ef8-932e-9bcefa142666"
      },
      "source": [
        "#### helper function\n",
        "Throughout this course, we will use OpenAI's `gpt-3.5-turbo` model and the [chat completions endpoint](https://platform.openai.com/docs/guides/chat).\n",
        "\n",
        "This helper function will make it easier to use prompts and look at the generated outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7dff174",
      "metadata": {
        "height": 166,
        "id": "a7dff174"
      },
      "outputs": [],
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "87121316",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 353,
        "id": "87121316",
        "outputId": "24eee590-2f0c-4319-eda4-1c1e67561341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To guide a model towards the desired output and reduce irrelevant or incorrect responses, it is important to provide clear and specific instructions, which can be achieved through longer prompts that offer more clarity and context.\n"
          ]
        }
      ],
      "source": [
        "text = f\"\"\"\n",
        "You should express what you want a model to do by \\\n",
        "providing instructions that are as clear and \\\n",
        "specific as you can possibly make them. \\\n",
        "This will guide the model towards the desired output, \\\n",
        "and reduce the chances of receiving irrelevant \\\n",
        "or incorrect responses. Don't confuse writing a \\\n",
        "clear prompt with writing a short prompt. \\\n",
        "In many cases, longer prompts provide more clarity \\\n",
        "and context for the model, which can lead to \\\n",
        "more detailed and relevant outputs.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "Summarize the text delimited by triple backticks \\\n",
        "into a single sentence.\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2798f3d-7618-4ac5-a6b2-3c69c537903d",
      "metadata": {
        "id": "f2798f3d-7618-4ac5-a6b2-3c69c537903d"
      },
      "source": [
        "#### Tactic 2: Ask for a structured output\n",
        "- JSON, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "6b50bbbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 166,
        "id": "6b50bbbd",
        "outputId": "978b444d-9b8e-45db-c511-3146d1de6c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"books\": [\n",
            "    {\n",
            "      \"book_id\": 1,\n",
            "      \"title\": \"The Enigma of Elysium\",\n",
            "      \"author\": \"Evelyn Sinclair\",\n",
            "      \"genre\": \"Mystery\"\n",
            "    },\n",
            "    {\n",
            "      \"book_id\": 2,\n",
            "      \"title\": \"Whispers in the Wind\",\n",
            "      \"author\": \"Nathaniel Blackwood\",\n",
            "      \"genre\": \"Fantasy\"\n",
            "    },\n",
            "    {\n",
            "      \"book_id\": 3,\n",
            "      \"title\": \"Echoes of the Past\",\n",
            "      \"author\": \"Amelia Hart\",\n",
            "      \"genre\": \"Romance\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Generate a list of three made-up book titles along \\\n",
        "with their authors and genres.\n",
        "Provide them in JSON format with the following keys:\n",
        "book_id, title, author, genre.\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22a71c4f-b1f1-4d67-ad5a-e49fc1e3147d",
      "metadata": {
        "id": "22a71c4f-b1f1-4d67-ad5a-e49fc1e3147d"
      },
      "source": [
        "#### Tactic 3: Ask the model to check whether conditions are satisfied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "f0ae612e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 523,
        "id": "f0ae612e",
        "outputId": "1bd4542b-b6cd-400d-e43c-6d85e8d552e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completion for Text 1:\n",
            "Step 1 - Get some water boiling.\n",
            "Step 2 - Grab a cup and put a tea bag in it.\n",
            "Step 3 - Once the water is hot enough, pour it over the tea bag.\n",
            "Step 4 - Let it sit for a bit so the tea can steep.\n",
            "Step 5 - After a few minutes, take out the tea bag.\n",
            "Step 6 - If you like, add some sugar or milk to taste.\n",
            "Step 7 - Enjoy your delicious cup of tea.\n"
          ]
        }
      ],
      "source": [
        "text_1 = f\"\"\"\n",
        "Making a cup of tea is easy! First, you need to get some \\\n",
        "water boiling. While that's happening, \\\n",
        "grab a cup and put a tea bag in it. Once the water is \\\n",
        "hot enough, just pour it over the tea bag. \\\n",
        "Let it sit for a bit so the tea can steep. After a \\\n",
        "few minutes, take out the tea bag. If you \\\n",
        "like, you can add some sugar or milk to taste. \\\n",
        "And that's it! You've got yourself a delicious \\\n",
        "cup of tea to enjoy.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "You will be provided with text delimited by triple quotes.\n",
        "If it contains a sequence of instructions, \\\n",
        "re-write those instructions in the following format:\n",
        "\n",
        "Step 1 - ...\n",
        "Step 2 - …\n",
        "…\n",
        "Step N - …\n",
        "\n",
        "If the text does not contain a sequence of instructions, \\\n",
        "then simply write \\\"No steps provided.\\\"\n",
        "\n",
        "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(\"Completion for Text 1:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "76b6cc59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 523,
        "id": "76b6cc59",
        "outputId": "586feb44-cc3b-4a2f-ec0a-dd29acf39f57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completion for Text 2:\n",
            "No steps provided.\n"
          ]
        }
      ],
      "source": [
        "text_2 = f\"\"\"\n",
        "The sun is shining brightly today, and the birds are \\\n",
        "singing. It's a beautiful day to go for a \\\n",
        "walk in the park. The flowers are blooming, and the \\\n",
        "trees are swaying gently in the breeze. People \\\n",
        "are out and about, enjoying the lovely weather. \\\n",
        "Some are having picnics, while others are playing \\\n",
        "games or simply relaxing on the grass. It's a \\\n",
        "perfect day to spend time outdoors and appreciate the \\\n",
        "beauty of nature.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "You will be provided with text delimited by triple quotes.\n",
        "If it contains a sequence of instructions, \\\n",
        "re-write those instructions in the following format:\n",
        "\n",
        "Step 1 - ...\n",
        "Step 2 - …\n",
        "…\n",
        "Step N - …\n",
        "\n",
        "If the text does not contain a sequence of instructions, \\\n",
        "then simply write \\\"No steps provided.\\\"\n",
        "\n",
        "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(\"Completion for Text 2:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c5866b8-d8c7-4e19-93db-401315f64954",
      "metadata": {
        "id": "3c5866b8-d8c7-4e19-93db-401315f64954"
      },
      "source": [
        "#### Tactic 4: \"Few-shot\" prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "82ce1540",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 268,
        "id": "82ce1540",
        "outputId": "b6d8424b-b2bc-43bb-a890-c5dc40025b23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<grandparent>: Resilience is like a mighty oak tree that withstands the strongest storms, bending but never breaking. It is the unwavering determination to rise again after every fall, and the ability to find strength in the face of adversity. Just as a diamond is formed under immense pressure, resilience is forged through challenges and hardships, making us stronger and more resilient in the process.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to answer in a consistent style.\n",
        "\n",
        "<child>: Teach me about patience.\n",
        "\n",
        "<grandparent>: The river that carves the deepest \\\n",
        "valley flows from a modest spring; the \\\n",
        "grandest symphony originates from a single note; \\\n",
        "the most intricate tapestry begins with a solitary thread.\n",
        "\n",
        "<child>: Teach me about resilience.\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece7a8ee-1a2d-415d-8c10-500ecff24b10",
      "metadata": {
        "id": "ece7a8ee-1a2d-415d-8c10-500ecff24b10"
      },
      "source": [
        "#### Tactic 5: Specify the steps required to complete a task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "5e7d6860",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 523,
        "id": "5e7d6860",
        "outputId": "7d7fd2b6-d775-4ca8-a55c-db6b566591a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completion for prompt 1:\n",
            "1 - Jack and Jill, siblings, go on a quest to fetch water from a hilltop well, but encounter misfortune when Jack trips on a stone and tumbles down the hill, with Jill following suit, yet they return home and remain undeterred in their adventurous spirits.\n",
            "\n",
            "2 - Jack et Jill, frère et sœur, partent en quête d'eau d'un puits au sommet d'une colline, mais rencontrent un malheur lorsque Jack trébuche sur une pierre et dévale la colline, suivi par Jill, pourtant ils rentrent chez eux et restent déterminés dans leur esprit d'aventure.\n",
            "\n",
            "3 - Jack, Jill\n",
            "\n",
            "4 - {\n",
            "  \"french_summary\": \"Jack et Jill, frère et sœur, partent en quête d'eau d'un puits au sommet d'une colline, mais rencontrent un malheur lorsque Jack trébuche sur une pierre et dévale la colline, suivi par Jill, pourtant ils rentrent chez eux et restent déterminés dans leur esprit d'aventure.\",\n",
            "  \"num_names\": 2\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "text = f\"\"\"\n",
        "In a charming village, siblings Jack and Jill set out on \\\n",
        "a quest to fetch water from a hilltop \\\n",
        "well. As they climbed, singing joyfully, misfortune \\\n",
        "struck—Jack tripped on a stone and tumbled \\\n",
        "down the hill, with Jill following suit. \\\n",
        "Though slightly battered, the pair returned home to \\\n",
        "comforting embraces. Despite the mishap, \\\n",
        "their adventurous spirits remained undimmed, and they \\\n",
        "continued exploring with delight.\n",
        "\"\"\"\n",
        "# example 1\n",
        "prompt_1 = f\"\"\"\n",
        "Perform the following actions:\n",
        "1 - Summarize the following text delimited by triple \\\n",
        "backticks with 1 sentence.\n",
        "2 - Translate the summary into French.\n",
        "3 - List each name in the French summary.\n",
        "4 - Output a json object that contains the following \\\n",
        "keys: french_summary, num_names.\n",
        "\n",
        "Separate your answers with line breaks.\n",
        "\n",
        "Text:\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt_1)\n",
        "print(\"Completion for prompt 1:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0bb1dcf-95f5-4ee1-8c25-8b2abd5f0f0d",
      "metadata": {
        "id": "d0bb1dcf-95f5-4ee1-8c25-8b2abd5f0f0d"
      },
      "source": [
        "#### Ask for output in a specified format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "3e4222cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 370,
        "id": "3e4222cc",
        "outputId": "9184e407-9f55-43af-a730-0d76138469b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Completion for prompt 2:\n",
            "Summary: Jack and Jill, siblings, go on a quest to fetch water from a hilltop well but encounter misfortune along the way.\n",
            "\n",
            "Translation: Jack et Jill, frère et sœur, partent en quête d'eau d'un puits au sommet d'une colline mais rencontrent des malheurs en chemin.\n",
            "\n",
            "Names: Jack, Jill\n",
            "\n",
            "Output JSON: {\"french_summary\": \"Jack et Jill, frère et sœur, partent en quête d'eau d'un puits au sommet d'une colline mais rencontrent des malheurs en chemin.\", \"num_names\": 2}\n"
          ]
        }
      ],
      "source": [
        "prompt_2 = f\"\"\"\n",
        "Your task is to perform the following actions:\n",
        "1 - Summarize the following text delimited by\n",
        "  <> with 1 sentence.\n",
        "2 - Translate the summary into French.\n",
        "3 - List each name in the French summary.\n",
        "4 - Output a json object that contains the\n",
        "  following keys: french_summary, num_names.\n",
        "\n",
        "Use the following format:\n",
        "Text: <text to summarize>\n",
        "Summary: <summary>\n",
        "Translation: <summary translation>\n",
        "Names: <list of names in Italian summary>\n",
        "Output JSON: <json with summary and num_names>\n",
        "\n",
        "Text: <{text}>\n",
        "\"\"\"\n",
        "response = get_completion(prompt_2)\n",
        "print(\"\\nCompletion for prompt 2:\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fec80fdb-92db-48f6-8f1d-b03c26385bad",
      "metadata": {
        "id": "fec80fdb-92db-48f6-8f1d-b03c26385bad"
      },
      "source": [
        "#### Tactic 6: Instruct the model to work out its own solution before rushing to a conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "ff5cc985",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 438,
        "id": "ff5cc985",
        "outputId": "219bc294-862d-487e-eee1-0bafd55a5545"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The student's solution is correct. They correctly identified the costs for land, solar panels, and maintenance, and calculated the total cost as a function of the number of square feet.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Determine if the student's solution is correct or not.\n",
        "\n",
        "Question:\n",
        "I'm building a solar power installation and I need \\\n",
        " help working out the financials.\n",
        "- Land costs $100 / square foot\n",
        "- I can buy solar panels for $250 / square foot\n",
        "- I negotiated a contract for maintenance that will cost \\\n",
        "me a flat $100k per year, and an additional $10 / square \\\n",
        "foot\n",
        "What is the total cost for the first year of operations\n",
        "as a function of the number of square feet.\n",
        "\n",
        "Student's Solution:\n",
        "Let x be the size of the installation in square feet.\n",
        "Costs:\n",
        "1. Land cost: 100x\n",
        "2. Solar panel cost: 250x\n",
        "3. Maintenance cost: 100,000 + 100x\n",
        "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f322ebd9-0f8a-43aa-97fe-5eac70cdcc6a",
      "metadata": {
        "id": "f322ebd9-0f8a-43aa-97fe-5eac70cdcc6a"
      },
      "source": [
        "#### Note that the student's solution is actually not correct.\n",
        "#### We can fix this by instructing the model to work out its own solution first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "703f7003",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 1016,
        "id": "703f7003",
        "outputId": "cd383608-7d23-4c49-9884-f7d0430ab5be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To calculate the total cost for the first year of operations, we need to add up the costs of land, solar panels, and maintenance.\n",
            "\n",
            "1. Land cost: $100 / square foot\n",
            "The cost of land is $100 multiplied by the number of square feet.\n",
            "\n",
            "2. Solar panel cost: $250 / square foot\n",
            "The cost of solar panels is $250 multiplied by the number of square feet.\n",
            "\n",
            "3. Maintenance cost: $100,000 + $10 / square foot\n",
            "The maintenance cost is a flat fee of $100,000 per year, plus $10 multiplied by the number of square feet.\n",
            "\n",
            "Total cost: Land cost + Solar panel cost + Maintenance cost\n",
            "\n",
            "So the actual solution is:\n",
            "Total cost = (100 * x) + (250 * x) + (100,000 + (10 * x))\n",
            "\n",
            "Is the student's solution the same as the actual solution just calculated:\n",
            "No\n",
            "\n",
            "Student grade:\n",
            "Incorrect\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to determine if the student's solution \\\n",
        "is correct or not.\n",
        "To solve the problem do the following:\n",
        "- First, work out your own solution to the problem.\n",
        "- Then compare your solution to the student's solution \\\n",
        "and evaluate if the student's solution is correct or not.\n",
        "Don't decide if the student's solution is correct until\n",
        "you have done the problem yourself.\n",
        "\n",
        "Use the following format:\n",
        "Question:\n",
        "```\n",
        "question here\n",
        "```\n",
        "Student's solution:\n",
        "```\n",
        "student's solution here\n",
        "```\n",
        "Actual solution:\n",
        "```\n",
        "steps to work out the solution and your solution here\n",
        "```\n",
        "Is the student's solution the same as actual solution \\\n",
        "just calculated:\n",
        "```\n",
        "yes or no\n",
        "```\n",
        "Student grade:\n",
        "```\n",
        "correct or incorrect\n",
        "```\n",
        "\n",
        "Question:\n",
        "```\n",
        "I'm building a solar power installation and I need help \\\n",
        "working out the financials.\n",
        "- Land costs $100 / square foot\n",
        "- I can buy solar panels for $250 / square foot\n",
        "- I negotiated a contract for maintenance that will cost \\\n",
        "me a flat $100k per year, and an additional $10 / square \\\n",
        "foot\n",
        "What is the total cost for the first year of operations \\\n",
        "as a function of the number of square feet.\n",
        "```\n",
        "Student's solution:\n",
        "```\n",
        "Let x be the size of the installation in square feet.\n",
        "Costs:\n",
        "1. Land cost: 100x\n",
        "2. Solar panel cost: 250x\n",
        "3. Maintenance cost: 100,000 + 100x\n",
        "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
        "```\n",
        "Actual solution:\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a207eab-a1b1-47a5-b913-fe38086123d0",
      "metadata": {
        "id": "8a207eab-a1b1-47a5-b913-fe38086123d0"
      },
      "source": [
        "## Model Limitations: Hallucinations\n",
        "- Boie is a real company, the product name is not real."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "81c80919",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "height": 115,
        "id": "81c80919",
        "outputId": "d2e38019-c8ff-432c-8190-9755872011e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The AeroGlide UltraSlim Smart Toothbrush by Boie is a technologically advanced toothbrush designed to provide a superior brushing experience. Boie is a company known for its innovative oral care products, and the AeroGlide UltraSlim Smart Toothbrush is no exception.\n",
            "\n",
            "One of the standout features of this toothbrush is its ultra-slim design. The brush head is only 2mm thick, making it much thinner than traditional toothbrushes. This slim profile allows for better access to hard-to-reach areas of the mouth, ensuring a thorough and effective clean.\n",
            "\n",
            "The AeroGlide UltraSlim Smart Toothbrush also incorporates smart technology. It connects to a mobile app via Bluetooth, allowing users to track their brushing habits and receive personalized recommendations for improving their oral hygiene routine. The app provides real-time feedback on brushing technique, duration, and coverage, helping users to achieve optimal oral health.\n",
            "\n",
            "The toothbrush features soft, antimicrobial bristles made from a durable thermoplastic elastomer. These bristles are gentle on the gums and teeth, while also being effective at removing plaque and debris. The antimicrobial properties help to keep the brush head clean and hygienic between uses.\n",
            "\n",
            "Another notable feature of the AeroGlide UltraSlim Smart Toothbrush is its long battery life. It can last up to 30 days on a single charge, making it convenient for travel or everyday use without the need for frequent recharging.\n",
            "\n",
            "Overall, the AeroGlide UltraSlim Smart Toothbrush by Boie offers a combination of advanced technology, slim design, and effective cleaning capabilities. It is a great option for those looking to upgrade their oral care routine and achieve a healthier smile.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a354ce17",
      "metadata": {
        "id": "a354ce17"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69df1e7d",
      "metadata": {
        "id": "69df1e7d"
      },
      "source": [
        "# Exercise Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "C7UhOEKitzC5",
      "metadata": {
        "id": "C7UhOEKitzC5"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain\n",
        "!pip install -qU openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "uXj9u9ZPIE-z",
      "metadata": {
        "id": "uXj9u9ZPIE-z"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain import FewShotPromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''\n",
        "openai.api_key=''\n",
        "# Helper Function\n",
        "def get_completion(prompt, temperature=0, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b966d6",
      "metadata": {
        "id": "e8b966d6"
      },
      "source": [
        "### 1. `Inferring` you will infer sentiment and topics from product reviews and news articles.\n",
        "\n",
        "Review text.\n",
        "lamp_review = \"\"\"\n",
        "Needed a nice lamp for my bedroom, and this one had \\\n",
        "additional storage and not too high of a price point. \\\n",
        "Got it fast.  The string to our lamp broke during the \\\n",
        "transit and the company happily sent over a new one. \\\n",
        "Came within a few days as well. It was easy to put \\\n",
        "together.  I had a missing part, so I contacted their \\\n",
        "support and they very quickly got me the missing piece! \\\n",
        "Lumina seems to me to be a great company that cares \\\n",
        "about their customers and products!!\n",
        "\"\"\"\n",
        "\n",
        "####  Task1\n",
        "1) Write prompt for Sentiment (positive/negative)\n",
        "2) Write prompt Identify types of emotions\n",
        "3) Write prompt Identify anger\n",
        "4) Write prompt Extract product and company name from customer reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "TdKPBrBR02_B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TdKPBrBR02_B",
        "outputId": "61710590-9051-4381-a5e3-820e1f7811ab"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Positive'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lamp_review = \"\"\"\n",
        "Needed a nice lamp for my bedroom, and this one had \\\n",
        "additional storage and not too high of a price point. \\\n",
        "Got it fast.  The string to our lamp broke during the \\\n",
        "transit and the company happily sent over a new one. \\\n",
        "Came within a few days as well. It was easy to put \\\n",
        "together.  I had a missing part, so I contacted their \\\n",
        "support and they very quickly got me the missing piece! \\\n",
        "Lumina seems to me to be a great company that cares \\\n",
        "about their customers and products!!\n",
        "\"\"\"\n",
        "\n",
        "####  Task1\n",
        "# 1) Write prompt for Sentiment (positive/negative)\n",
        "sentiment_template = \"\"\"\n",
        "What is the sentiment of the following review delimited by triple backticks?\n",
        "\n",
        "Give answer in a single word \"Positive\" or \"Negative\".\n",
        "\n",
        "Review : ```{review}```\n",
        "\"\"\"\n",
        "sentiment_prompt_template = PromptTemplate(\n",
        "    template = sentiment_template,\n",
        "    input_variables = ['review']\n",
        ")\n",
        "lamp_sentiment_prompt = sentiment_prompt_template.format(\n",
        "    review = lamp_review\n",
        ")\n",
        "get_completion(\n",
        "    prompt = lamp_sentiment_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4sCcih_e6VIV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4sCcih_e6VIV",
        "outputId": "efaa2668-8286-4f21-f7af-b1f8a5df24cc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'satisfaction happiness gratitude'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2) Write prompt Identify types of emotions\n",
        "emotion_template = \"\"\"\n",
        "Identify the emotions expressed in the following review delimited by triple backticks.\n",
        "Format your output as a list seperated with spaces.\n",
        "\n",
        "Review : ```{review}```\n",
        "\"\"\"\n",
        "emotion_prompt_template = PromptTemplate(\n",
        "    template = emotion_template,\n",
        "    input_variables = ['review']\n",
        ")\n",
        "lamp_emotion_prompt = emotion_prompt_template.format(\n",
        "    review = lamp_review\n",
        ")\n",
        "get_completion(\n",
        "    prompt = lamp_emotion_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "QlAkE40b8C9I",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QlAkE40b8C9I",
        "outputId": "a7baeb8f-9631-4f27-8cda-6ba2063c01de"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'No'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3) Write prompt Identify anger\n",
        "emotion_template = \"\"\"\n",
        "Is there an expression of anger in the following review delimited by triple backticks?\n",
        "Give your answer as \"Yes\" or \"No\" only.\n",
        "\n",
        "Review : ```{review}```\n",
        "\"\"\"\n",
        "anger_prompt_template = PromptTemplate(\n",
        "    template = emotion_template,\n",
        "    input_variables = ['review']\n",
        ")\n",
        "lamp_anger_prompt = anger_prompt_template.format(\n",
        "    review = lamp_review\n",
        ")\n",
        "get_completion(\n",
        "    prompt = lamp_anger_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "vcgulR538wF7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vcgulR538wF7",
        "outputId": "b2c73478-2fd0-45a5-94f6-0758650e8b8e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\\n  \"Product\": \"lamp\",\\n  \"Company name\": \"Lumina\"\\n}'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 4) Write prompt Extract product and company name from customer reviews\n",
        "product_and_company_template = \"\"\"\n",
        "From the given review delimited by triple backticks extract:\n",
        "1. Product\n",
        "2. Company name\n",
        "If the names are not mentioned use 'unknown' as the answer.\n",
        "Format your answer in the form of dictionary.\n",
        "\n",
        "Review : ```{review}```\n",
        "\"\"\"\n",
        "product_and_company_prompt_template = PromptTemplate(\n",
        "    template = product_and_company_template,\n",
        "    input_variables = ['review']\n",
        ")\n",
        "lamp_product_and_company_prompt = product_and_company_prompt_template.format(\n",
        "    review = lamp_review\n",
        ")\n",
        "get_completion(\n",
        "    prompt = lamp_product_and_company_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2BZ3v-5X6IEB",
      "metadata": {
        "id": "2BZ3v-5X6IEB"
      },
      "source": [
        "story = \"\"\"\n",
        "In a recent survey conducted by the government,\n",
        "public sector employees were asked to rate their level\n",
        "of satisfaction with the department they work at.\n",
        "The results revealed that NASA was the most popular\n",
        "department with a satisfaction rating of 95%.\n",
        "\n",
        "One NASA employee, John Smith, commented on the findings,\n",
        "stating, \"I'm not surprised that NASA came out on top.\n",
        "It's a great place to work with amazing people and\n",
        "incredible opportunities. I'm proud to be a part of\n",
        "such an innovative organization.\"\n",
        "\n",
        "The results were also welcomed by NASA's management team,\n",
        "with Director Tom Johnson stating, \"We are thrilled to\n",
        "hear that our employees are satisfied with their work at NASA.\n",
        "We have a talented and dedicated team who work tirelessly\n",
        "to achieve our goals, and it's fantastic to see that their\n",
        "hard work is paying off.\"\n",
        "\n",
        "The survey also revealed that the\n",
        "Social Security Administration had the lowest satisfaction\n",
        "rating, with only 45% of employees indicating they were\n",
        "satisfied with their job. The government has pledged to\n",
        "address the concerns raised by employees in the survey and\n",
        "work towards improving job satisfaction across all departments.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "####  Task2\n",
        "\n",
        "1) Infer 5 topics that are being discussed in the given story"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "QGeKiwFm-8vA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QGeKiwFm-8vA",
        "outputId": "8007a75b-be56-4628-c08b-39b939580796"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'government survey\\nsatisfaction ratings\\nNASA popularity\\nSocial Security Administration\\njob satisfaction improvement'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "story = \"\"\"\n",
        "In a recent survey conducted by the government,\n",
        "public sector employees were asked to rate their level\n",
        "of satisfaction with the department they work at.\n",
        "The results revealed that NASA was the most popular\n",
        "department with a satisfaction rating of 95%.\n",
        "\n",
        "One NASA employee, John Smith, commented on the findings,\n",
        "stating, \"I'm not surprised that NASA came out on top.\n",
        "It's a great place to work with amazing people and\n",
        "incredible opportunities. I'm proud to be a part of\n",
        "such an innovative organization.\"\n",
        "\n",
        "The results were also welcomed by NASA's management team,\n",
        "with Director Tom Johnson stating, \"We are thrilled to\n",
        "hear that our employees are satisfied with their work at NASA.\n",
        "We have a talented and dedicated team who work tirelessly\n",
        "to achieve our goals, and it's fantastic to see that their\n",
        "hard work is paying off.\"\n",
        "\n",
        "The survey also revealed that the\n",
        "Social Security Administration had the lowest satisfaction\n",
        "rating, with only 45% of employees indicating they were\n",
        "satisfied with their job. The government has pledged to\n",
        "address the concerns raised by employees in the survey and\n",
        "work towards improving job satisfaction across all departments.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "####  Task2\n",
        "\n",
        "# 1) Infer 5 topics that are being discussed in the given story\n",
        "topics_template = \"\"\"\n",
        "Identify exactly 5 topics discussed in the given story delimited by triple backticks.\n",
        "Each topic must be under 3 words.\n",
        "Separate the topics with line breaks.\n",
        "\n",
        "Story : ```{story}```\n",
        "\"\"\"\n",
        "topics_prompt_template = PromptTemplate(\n",
        "    template = topics_template,\n",
        "    input_variables = ['story']\n",
        ")\n",
        "topics_prompt = topics_prompt_template.format(\n",
        "    story = story\n",
        ")\n",
        "get_completion(\n",
        "    prompt = topics_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e74ad6e",
      "metadata": {
        "id": "5e74ad6e"
      },
      "source": [
        "## 2 `Expanding` you will generate customer service emails that are tailored to each customer's review.\n",
        "\n",
        "review = \"\"\"So, they still had the 17 piece system on seasonal \\\n",
        "sale for around 49 dollar in the month of November, about \\\n",
        "half off, but for some reason (call it price gouging) \\\n",
        "around the second week of December the prices all went \\\n",
        "up to about anywhere from between 70-89 dollar for the same \\\n",
        "system. And the 11 piece system went up around 10 dollar or \\\n",
        "so in price also from the earlier sale price of 29 dollar. \\\n",
        "So it looks okay, but if you look at the base, the part \\\n",
        "where the blade locks into place doesn’t look as good \\\n",
        "as in previous editions from a few years ago, but I \\\n",
        "plan to be very gentle with it (example, I crush \\\n",
        "very hard items like beans, ice, rice, etc. in the \\\n",
        "blender first then pulverize them in the serving size \\\n",
        "I want in the blender then switch to the whipping \\\n",
        "blade for a finer flour, and use the cross cutting blade \\\n",
        "first when making smoothies, then use the flat blade \\\n",
        "if I need them finer/less pulpy). Special tip when making \\\n",
        "smoothies, finely cut and freeze the fruits and \\\n",
        "vegetables (if using spinach-lightly stew soften the \\\n",
        "spinach then freeze until ready for use-and if making \\\n",
        "sorbet, use a small to medium sized food processor) \\\n",
        "that you plan to use that way you can avoid adding so \\\n",
        "much ice if at all-when making your smoothie. \\\n",
        "After about a year, the motor was making a funny noise. \\\n",
        "I called customer service but the warranty expired \\\n",
        "already, so I had to buy another one. FYI: The overall \\\n",
        "quality has gone down in these types of products, so \\\n",
        "they are kind of counting on brand recognition and \\\n",
        "consumer loyalty to maintain sales. Got it in about \\\n",
        "two days.\"\"\"\n",
        "\n",
        "####  Task1\n",
        "1) Write prompt for Customize the automated reply to a customer email and remind the model to use details from the customer's email\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a6ec906b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "a6ec906b",
        "outputId": "8f02df65-bb05-4c7c-9e16-da121e9e9633"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Dear Customer,\\n\\nThank you for taking the time to provide feedback on your recent purchase of our 17 piece system. We appreciate your insights and concerns regarding the pricing changes and the quality of the base.\\n\\nRegarding the pricing, I apologize for any confusion caused. Our seasonal sale in November offered the 17 piece system at a discounted price of around $49, which was indeed a significant reduction from the original price. However, I understand your disappointment when you noticed the prices increased to around $70-89 in the second week of December. Please be assured that we strive to offer competitive prices while considering various factors such as production costs and market demand. I apologize if this was not communicated clearly.\\n\\nRegarding the quality of the base, I understand your observation that it may not appear as good as previous editions. We constantly strive to improve our products based on customer feedback, and I will ensure that your concerns are shared with our product development team for further assessment.\\n\\nI appreciate your valuable tips and suggestions on using the blender effectively, especially when making smoothies. Your insights will be helpful for other customers as well.\\n\\nI apologize for the inconvenience you experienced with the motor making a funny noise after a year. Our warranty does cover such issues, but I understand that it had already expired in your case. I apologize for any inconvenience caused. Should you encounter any further issues, please do not hesitate to reach out to our customer service team, and we will be more than happy to assist you.\\n\\nOnce again, thank you for your feedback and for choosing our product. We value your loyalty and will continue working towards providing you with the best possible products and services.\\n\\nBest regards,\\n[Your Name]\\nCustomer Service Representative'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#### Code ####\n",
        "review = \"\"\"So, they still had the 17 piece system on seasonal \\\n",
        "sale for around 49 dollar in the month of November, about \\\n",
        "half off, but for some reason (call it price gouging) \\\n",
        "around the second week of December the prices all went \\\n",
        "up to about anywhere from between 70-89 dollar for the same \\\n",
        "system. And the 11 piece system went up around 10 dollar or \\\n",
        "so in price also from the earlier sale price of 29 dollar. \\\n",
        "So it looks okay, but if you look at the base, the part \\\n",
        "where the blade locks into place doesn’t look as good \\\n",
        "as in previous editions from a few years ago, but I \\\n",
        "plan to be very gentle with it (example, I crush \\\n",
        "very hard items like beans, ice, rice, etc. in the \\\n",
        "blender first then pulverize them in the serving size \\\n",
        "I want in the blender then switch to the whipping \\\n",
        "blade for a finer flour, and use the cross cutting blade \\\n",
        "first when making smoothies, then use the flat blade \\\n",
        "if I need them finer/less pulpy). Special tip when making \\\n",
        "smoothies, finely cut and freeze the fruits and \\\n",
        "vegetables (if using spinach-lightly stew soften the \\\n",
        "spinach then freeze until ready for use-and if making \\\n",
        "sorbet, use a small to medium sized food processor) \\\n",
        "that you plan to use that way you can avoid adding so \\\n",
        "much ice if at all-when making your smoothie. \\\n",
        "After about a year, the motor was making a funny noise. \\\n",
        "I called customer service but the warranty expired \\\n",
        "already, so I had to buy another one. FYI: The overall \\\n",
        "quality has gone down in these types of products, so \\\n",
        "they are kind of counting on brand recognition and \\\n",
        "consumer loyalty to maintain sales. Got it in about \\\n",
        "two days.\"\"\"\n",
        "\n",
        "review_template = \"\"\"\n",
        "Given the following review delimited by triple backticks your task is to send an email reply to the customer.\n",
        "Make sure to use specific details from the review.\n",
        "Write in a concise and professional tone.\n",
        "\n",
        "Review : ```{review}```\n",
        "\"\"\"\n",
        "review_prompt_template = PromptTemplate(\n",
        "    template = review_template,\n",
        "    input_variables = ['review']\n",
        ")\n",
        "review_prompt = review_prompt_template.format(\n",
        "    review = review\n",
        ")\n",
        "get_completion(\n",
        "    prompt = review_prompt,\n",
        "    temperature = 0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4592125f",
      "metadata": {
        "id": "4592125f"
      },
      "source": [
        "## 3 Summarizing you will summarize text with a focus on specific topics.\n",
        "\n",
        "prod_review = \"\"\"\n",
        "Got this panda plush toy for my daughter's birthday, \\\n",
        "who loves it and takes it everywhere. It's soft and \\\n",
        "super cute, and its face has a friendly look. It's \\\n",
        "a bit small for what I paid though. I think there \\\n",
        "might be other options that are bigger for the \\\n",
        "same price. It arrived a day earlier than expected, \\\n",
        "so I got to play with it myself before I gave it \\\n",
        "to her.\n",
        "\"\"\"\n",
        "\n",
        "### Task\n",
        "\n",
        "1) Write a prompt to summarize it with focus on delivery and shipping\n",
        "2) Write a prompt to summarize it with focus on price and value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c1ff315b",
      "metadata": {
        "id": "c1ff315b"
      },
      "outputs": [],
      "source": [
        "prod_review = \"\"\"\n",
        "Got this panda plush toy for my daughter's birthday, \\\n",
        "who loves it and takes it everywhere. It's soft and \\\n",
        "super cute, and its face has a friendly look. It's \\\n",
        "a bit small for what I paid though. I think there \\\n",
        "might be other options that are bigger for the \\\n",
        "same price. It arrived a day earlier than expected, \\\n",
        "so I got to play with it myself before I gave it \\\n",
        "to her.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "i17cSOWPQWd0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "i17cSOWPQWd0",
        "outputId": "8b20154e-4a50-402d-b947-221863082a43"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The product arrived a day earlier than expected, allowing the reviewer to play with it before giving it to their daughter.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#1) Write a prompt to summarize it with focus on delivery and shipping\n",
        "\n",
        "delivery_and_ship_template = \"\"\"\n",
        "Your task is to extract relevant information from a product review only with a specific focus on the delivery and shipping.\n",
        "The response can not exceed 25 words. The review is delimited by triple backticks.\n",
        "\n",
        "Review : ```{prod_review}```\n",
        "\"\"\"\n",
        "delivery_and_ship_prompt_template = PromptTemplate(\n",
        "    template = delivery_and_ship_template,\n",
        "    input_variables = ['prod_review']\n",
        ")\n",
        "delivery_and_ship_prompt = delivery_and_ship_prompt_template.format(\n",
        "    prod_review = prod_review\n",
        ")\n",
        "get_completion(\n",
        "    prompt = delivery_and_ship_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "IXdcre2SQYqw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IXdcre2SQYqw",
        "outputId": "68a27a90-8122-41dd-ed4a-39ed939a295a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The panda plush toy is loved by the daughter, but the reviewer feels it is small for the price.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#2) Write a prompt to summarize it with focus on price and value\n",
        "price_and_value_template = \"\"\"\n",
        "Your task is to extract relevant information from a product review only with a specific focus on the price and value.\n",
        "The response can not exceed 25 words. The review is delimited by triple backticks.\n",
        "\n",
        "Review : ```{prod_review}```\n",
        "\"\"\"\n",
        "price_and_value_prompt_template = PromptTemplate(\n",
        "    template = price_and_value_template,\n",
        "    input_variables = ['prod_review']\n",
        ")\n",
        "price_and_value_prompt = price_and_value_prompt_template.format(\n",
        "    prod_review = prod_review\n",
        ")\n",
        "get_completion(\n",
        "    prompt = price_and_value_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16b1fd0b",
      "metadata": {
        "id": "16b1fd0b"
      },
      "source": [
        "# Transforming use Large Language Models for text transformation tasks such as language translation, spelling and grammar checking, tone adjustment, and format conversion.\n",
        "\n",
        "\n",
        "\n",
        "## Task 1\n",
        "'Dude, This is Joe, check out this spec on this standing lamp.'\n",
        "1) Write prompt to tansform this text into formal tone\n",
        "\n",
        "2) Write a prompt to transform and check for the spelling and Homonyms\n",
        "\n",
        "    Text =\n",
        "  \"The girl with the black and white puppies have a ball.\",  # The girl has a ball.\n",
        "  \"Yolanda has her notebook.\", # ok\n",
        "  \"Its going to be a long day. Does the car need it’s oil changed?\",  # Homonyms\n",
        "  \"Their goes my freedom. There going to bring they’re suitcases.\",  # Homonyms\n",
        "  \"Your going to need you’re notebook.\",  # Homonyms\n",
        "  \"That medicine effects my ability to sleep. Have you heard of the butterfly affect?\", # Homonyms\n",
        "  \"This phrase is to cherck chatGPT for speling abilitty\"  # spelling\n",
        "  \n",
        "3) Write a prompt to translate all given text to english\n",
        "\n",
        "text =   \"La performance du système est plus lente que d'habitude.\",  \n",
        "  \"Mi monitor tiene píxeles que no se iluminan.\",              \n",
        "  \"Il mio mouse non funziona\",                               \n",
        "  \"Mój klawisz Ctrl jest zepsuty\",                            \n",
        "  \"我的屏幕在闪烁\"                                               \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "Ywp9hJBALvv9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ywp9hJBALvv9",
        "outputId": "c0989fd8-26b8-4816-e958-448dffdf1b1f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hello, this is Joe. I would like to bring your attention to the specifications of this standing lamp.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = 'Dude, This is Joe, check out this spec on this standing lamp.'\n",
        "\n",
        "# 1) Write prompt to tansform this text into formal tone\n",
        "\n",
        "tone_template = \"\"\"\n",
        "Transform the following text delimited by triple backticks to formal tone :\n",
        "```{text}```\n",
        "\"\"\"\n",
        "tone_prompt_template = PromptTemplate(\n",
        "    template = tone_template,\n",
        "    input_variables = ['text']\n",
        ")\n",
        "tone_prompt = tone_prompt_template.format(\n",
        "    text = text\n",
        ")\n",
        "get_completion(\n",
        "    prompt = tone_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "K70CxaWMNMGf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "K70CxaWMNMGf",
        "outputId": "3792b134-dce8-48fa-b3dd-2806621d9784"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The girl with the black and white puppies has a ball.\\nThe girl has a ball.\\nYolanda has her notebook.\\nIt's going to be a long day. Does the car need its oil changed?\\nThere goes my freedom. They're going to bring their suitcases.\\nYou're going to need your notebook.\\nThat medicine affects my ability to sleep. Have you heard of the butterfly effect?\\nThis phrase is to check chatGPT for spelling ability.\""
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2) Write a prompt to transform and check for the spelling and Homonyms\n",
        "\n",
        "text = [\n",
        "  \"The girl with the black and white puppies have a ball.\",\n",
        "  \"The girl has a ball.\",\n",
        "  \"Yolanda has her notebook.\",\n",
        "  \"Its going to be a long day. Does the car need it’s oil changed?\",\n",
        "  \"Their goes my freedom. There going to bring they’re suitcases.\",\n",
        "  \"Your going to need you’re notebook.\",\n",
        "  \"That medicine effects my ability to sleep. Have you heard of the butterfly affect?\",\n",
        "  \"This phrase is to cherck chatGPT for speling abilitty\"\n",
        "]\n",
        "\n",
        "\n",
        "spell_template = \"\"\"\n",
        "Given the following text delimited by triple backticks proofread and correct the text and rewrite the corrected versions.\n",
        "If no mistakes are found just say \"No mistake found\".\n",
        "Separate your answers with line breaks.\n",
        "\n",
        "Text : {text}\n",
        "\"\"\"\n",
        "spell_prompt_template = PromptTemplate(\n",
        "    template = spell_template,\n",
        "    input_variables = ['text']\n",
        ")\n",
        "spell_prompt = spell_prompt_template.format(\n",
        "    text = text\n",
        ")\n",
        "get_completion(\n",
        "    prompt = spell_prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "9951f7aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9951f7aa",
        "outputId": "88c968a8-c54e-4459-9f6d-6497f1774879"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The system performance is slower than usual.\\nMy monitor has pixels that do not light up.\\nMy mouse is not working.\\nMy Ctrl key is broken.\\nMy screen is flickering.'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3) Write a prompt to translate all given text to english\n",
        "\n",
        "text =   [\n",
        "  \"La performance du système est plus lente que d'habitude.\",\n",
        "  \"Mi monitor tiene píxeles que no se iluminan.\",\n",
        "  \"Il mio mouse non funziona\",\n",
        "  \"Mój klawisz Ctrl jest zepsuty\",\n",
        "  \"我的屏幕在闪烁\"\n",
        "]\n",
        "\n",
        "translate_template = \"\"\"\n",
        "Translate the following text delimited by triple backticks to English :\n",
        "```{text}```\n",
        "\n",
        "Separate your answers with line breaks.\n",
        "\"\"\"\n",
        "translate_prompt_template = PromptTemplate(\n",
        "    template = translate_template,\n",
        "    input_variables = ['text']\n",
        ")\n",
        "translate_prompt = translate_prompt_template.format(\n",
        "    text = text\n",
        ")\n",
        "get_completion(\n",
        "    prompt = translate_prompt\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
