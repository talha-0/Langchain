{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVDmZr57I6d5"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmnTnKECjIx-"
      },
      "source": [
        "# Import libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kKtMIZR9IZcs"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "from langchain import OpenAI, PromptTemplate\n",
        "from langchain.chains import LLMChain,ConversationChain\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CI1iTtzId5c",
        "outputId": "4f52265b-3d8c-4390-fd67-4fb89c7d5ef9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "OPENAI_API_KEY = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F5vMwuWFRqkq"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6jt4FnfjcCU"
      },
      "source": [
        "# Required prompt templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n95P7RwzRhLb"
      },
      "outputs": [],
      "source": [
        "qa_template = \"\"\"Answer the questions in the context given below.\\\n",
        "Context: {chat_history}\n",
        "Keep the answer short and concise. \\\n",
        "Respond \"Unsure about answer\" if not sure about the answer.\\\n",
        "Question: {input}\"\"\"\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "{chat_history}\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "bash_template = \"\"\"If someone asks you to perform a task, \\\n",
        "your job is to come up with a series of bash commands \\\n",
        "that will perform the task. There is no need to \\\n",
        "put \\\"#!/bin/bash\\\" in your answer. Make sure to \\\n",
        "reason step by step, using this format:\n",
        "\\n\\nQuestion: \\\"copy the files in the directory named \\\n",
        "'target' into a new directory at the same level as \\\n",
        "target called 'myNewDirectory'\\\n",
        "\\\"\\n\\nI need to take the following actions:\n",
        "\\n- List all files in the directory\n",
        "\\n- Create a new directory\n",
        "\\n- Copy the files from the first directory into the second directory\n",
        "\\n```bash\\nls\\nmkdir myNewDirectory\\ncp -r target/* myNewDirectory\\n```\\n\\n\n",
        "That is the format. Begin!\n",
        "{chat_history}\n",
        "\\n\\nQuestion: {input}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cFCMg8Mt9FxE"
      },
      "outputs": [],
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"bash\",\n",
        "        \"description\": \"Good for writing bash script\",\n",
        "        \"prompt_template\": bash_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"qa\",\n",
        "        \"description\": \"Good for answering generic questions\",\n",
        "        \"prompt_template\": qa_template\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9ORswZJJNYdS"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(openai_api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK8VjhjGjixl"
      },
      "source": [
        "# Initializing destination chains with memories\n",
        "\n",
        "* verbose=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FSEJde2SPr1H"
      },
      "outputs": [],
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
        "    chain = LLMChain(\n",
        "        verbose=True,\n",
        "        llm=llm,\n",
        "        prompt=prompt,\n",
        "        memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "    )\n",
        "    destination_chains[name] = chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0xRXe1R5dL1i"
      },
      "outputs": [],
      "source": [
        "default_chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    output_key=\"text\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "R56dAivXS6fS"
      },
      "outputs": [],
      "source": [
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRFXckaSjrve"
      },
      "source": [
        "# Router Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BEfUJ8WWSZ4T"
      },
      "outputs": [],
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMtOcRcUj1Iu"
      },
      "source": [
        "# Chain to route to destination chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "I4fkdVsTQOHq"
      },
      "outputs": [],
      "source": [
        "from langchain import memory\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "router_chain = LLMRouterChain.from_llm(\n",
        "    llm,\n",
        "    router_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbhUc6ZhjxKh"
      },
      "source": [
        "# Final Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "j9-0VRthQepL"
      },
      "outputs": [],
      "source": [
        "chain = MultiPromptChain(\n",
        "    router_chain=router_chain,\n",
        "    destination_chains=destination_chains,\n",
        "    default_chain=default_chain,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "eB1bV9dgRvIA",
        "outputId": "730de275-5b2b-4c2a-f854-4a308a385ffe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "qa: {'input': 'What is black body radiation?'}\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAnswer the questions in the context given below.Context: \n",
            "Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.Question: What is black body radiation?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nBlack body radiation is the thermal electromagnetic radiation emitted from the surface of a body that is in thermal equilibrium with its environment.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run(\"What is black body radiation?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "Uc69PKdOR_b8",
        "outputId": "e60c2a54-e00a-48a4-9f10-521e9412f8c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bash: {'input': 'mkdir [name of directory]'}\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mIf someone asks you to perform a task, your job is to come up with a series of bash commands that will perform the task. There is no need to put \"#!/bin/bash\" in your answer. Make sure to reason step by step, using this format:\n",
            "\n",
            "\n",
            "Question: \"copy the files in the directory named 'target' into a new directory at the same level as target called 'myNewDirectory'\"\n",
            "\n",
            "I need to take the following actions:\n",
            "\n",
            "- List all files in the directory\n",
            "\n",
            "- Create a new directory\n",
            "\n",
            "- Copy the files from the first directory into the second directory\n",
            "\n",
            "```bash\n",
            "ls\n",
            "mkdir myNewDirectory\n",
            "cp -r target/* myNewDirectory\n",
            "```\n",
            "\n",
            "\n",
            "That is the format. Begin!\n",
            "\n",
            "\n",
            "\n",
            "Question: mkdir [name of directory]\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n```bash\\nmkdir [name of directory]\\n```'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run(\"How to make a new dir\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "4KxEWie9b743",
        "outputId": "0bcb4728-dfc2-4527-d9e2-c4114c5a9ed5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "qa: {'input': 'Is it related to electromagnetic radiation?'}\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAnswer the questions in the context given below.Context: Human: What is black body radiation?\n",
            "AI: \n",
            "\n",
            "Black body radiation is the thermal electromagnetic radiation emitted from the surface of a body that is in thermal equilibrium with its environment.\n",
            "Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.Question: Is it related to electromagnetic radiation?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nYes, it is related to electromagnetic radiation.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run(\"Is it related to electromagnetic radiation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "O_k2aF23feOH",
        "outputId": "718ef4ab-74fa-4972-a774-a2c702b8730c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "qa: {'input': 'What is it related to?'}\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAnswer the questions in the context given below.Context: Human: What is black body radiation?\n",
            "AI: \n",
            "\n",
            "Black body radiation is the thermal electromagnetic radiation emitted from the surface of a body that is in thermal equilibrium with its environment.\n",
            "Human: Is it related to electromagnetic radiation?\n",
            "AI: \n",
            "\n",
            "Yes, it is related to electromagnetic radiation.\n",
            "Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.Question: What is it related to?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nIt is related to electromagnetic radiation.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run(\"What is it related to\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "f2lLSLMwcF43",
        "outputId": "a4043df4-a2c0-4c75-b967-f459073796ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "qa: {'input': 'What are we talking about?'}\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAnswer the questions in the context given below.Context: Human: What is black body radiation?\n",
            "AI: \n",
            "\n",
            "Black body radiation is the thermal electromagnetic radiation emitted from the surface of a body that is in thermal equilibrium with its environment.\n",
            "Human: Is it related to electromagnetic radiation?\n",
            "AI: \n",
            "\n",
            "Yes, it is related to electromagnetic radiation.\n",
            "Human: What is it related to?\n",
            "AI: \n",
            "\n",
            "It is related to electromagnetic radiation.\n",
            "Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.Question: What are we talking about?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nWe are talking about black body radiation.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run(\"What are we taking about\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
